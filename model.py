# -*- coding: utf-8 -*-
"""Hand-Recog.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KLVuIWwV_IZetSxk-Xju7KYXkm2hWLM9

# Intro
The Hand Gesture Recognition Database is a collection of near-infra-red images of ten distinct hand gestures. In this notebook we use end-to-end deep learning to build a classifier for these images.

We'll first load some packages required for reading in and plotting the images.
"""

!nvidia-smi

from google.colab import files

import warnings
warnings.simplefilter("ignore")

!pip install -q kaggle
files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d sarjit07/hand-gesture-recog-dataset

!unzip -q hand-gesture-recog-dataset.zip -d HandGest

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import os
import tensorflow as tf
import cv2
import random
from tensorflow import keras
from tensorflow.keras.layers import Dense, Input, InputLayer, Flatten
from tensorflow.keras import models, layers
from tensorflow.keras.models import Sequential, Model
from  matplotlib import pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline

plt.figure(figsize=(20,20))
img_folder="HandGest/data/thumbsup"
for i in range(5):
    file = random.choice(os.listdir(img_folder))
    image_path= os.path.join(img_folder, file)
    img=mpimg.imread(image_path)
    ax=plt.subplot(1,5,i+1)
    ax.title.set_text(file)
    plt.imshow(img)

IMG_WIDTH=125
IMG_HEIGHT=125
img_folder='HandGest/data'

!rm -r 'HandGest/data/blank'

def create_dataset(img_folder):
   
    img_data_array=[]
    class_name=[]
   
    for dir1 in os.listdir(img_folder):
        for file in os.listdir(os.path.join(img_folder, dir1)):
       
            image_path= os.path.join(img_folder, dir1,  file)
            image= cv2.imread( image_path, cv2.COLOR_BGR2RGB)
            image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)
            image=np.array(image)
            image = image.astype('float32')
            image /= 255 
            image = np.stack((image,)*3, axis=-1)
            img_data_array.append(image)
            class_name.append(dir1)
    return img_data_array, class_name
    
# extract the image array and class name
img_data, class_name =create_dataset(img_folder)

target_dict={k: v for v, k in enumerate(np.unique(class_name))}
print(target_dict)
target_val= [target_dict[class_name[i]] for i in range(len(class_name))]

x_data = np.array(img_data, np.float32)
y_data = np.array(list(map(int,target_val)))
y_data = y_data.reshape(y_data.shape[0],1)
print(x_data.shape, y_data.shape)

from sklearn.model_selection import train_test_split
x_train,x_further,y_train,y_further = train_test_split(x_data,y_data,test_size = 0.2)
x_validate,x_test,y_validate,y_test = train_test_split(x_further,y_further,test_size = 0.5)

model=models.Sequential()
model.add(layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu', input_shape=(125, 125,3))) 
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu')) 
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(5, activation='softmax'))
model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

history = model.fit(x_train, y_train, epochs=10, batch_size=16, verbose=1, validation_data=(x_validate, y_validate))

[loss, acc] = model.evaluate(x_test,y_test,verbose=1)
print("Accuracy:" + str(acc))

"""You'll get slightly different numbers each time you run it but you should be getting between 99.9 and 100% accuracy. Great!"""

image = cv2.imread("/content/HandGest/data/thumbsdown/down0.jpg")
image = cv2.resize(image, (125,125))
image=np.array(image)
image = image.astype('float32')
image /= 255 
image = image.reshape(-1,125,125,3)
image.shape
np.argmax(model.predict(image), axis=-1)[0]

model.save("CNN.h5")

